{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Tree Learning: Exercise 1\n",
    "\n",
    "\n",
    "\n",
    "In this practical you will have your first real contact with supervised machine learning applied to real biological data. \n",
    "\n",
    "Your task is to establish, which biomarkers (or features/attributes) influence the outcome. This execise goes through the clinical biomarkers and you will have a look at the data using decision trees and random forrests. The author of the paper (see below) has established that no real clinical biomarkers could be found. Instead, he found some other biomarkers, which will be part of the  second excerice. The file \n",
    "\n",
    "```\n",
    "'clinical_biomarkers.csv'\n",
    "``` \n",
    "\n",
    "Contains those clinical biomarkers.\n",
    "\n",
    "Please go through the exercise/tutorial and establish that you know what you are doing. In the second exercise you will have a look into the informative (genetic) biomarkers. \n",
    "\n",
    "For each exercise: Which features are the most informatives?\n",
    "\n",
    "\n",
    "\n",
    "## Data origin\n",
    "\n",
    "The data originates form the following publication:\n",
    "\n",
    "https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1292-2\n",
    "\n",
    "(going down to section Additional files - Additional file 3 will give you the full ist of raw data)\n",
    "\n",
    "For the purpose of the exercise, we transformed the data already.\n",
    "\n",
    "Before goint into downloading the data - some common imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting and visulisation\n",
    "import seaborn as sns # nicer (easier) visualisation\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some required import for plotting a learnt tree \n",
    "\n",
    "Here, you can load a small helper file, allowing to plot the learnt tree using a programme called graphviz.  \n",
    "This library assume that graphviz is installed locally (which is true for the Jupyer Lab environment on bearportal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own mini- library\n",
    "import session_helpers\n",
    "import IPython.display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the file and setting the first column to be the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomarkers_file_csv = 'clinical_biomarkers.csv'\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(biomarkers_file_csv)\n",
    "df = df.set_index(['Sample'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task For You \n",
    "- Please have a look at the loaded data. How many columns/attributes does it have? \n",
    "- What can you say about the values in each of the columns? Just have a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping classes into positive and negative\n",
    "\n",
    "The following maps examples either to be positive or negative. \n",
    "You can exclude a set of example by putting them into comments (just let the start of the line be an ```#``` - see for example ```Low```in the code below.). If there is no mapping, the values will be set to ```None``` or ```NaN``` (the numpy version of ```None```).  \n",
    "\n",
    "The final line in the code, keeps only entries, which have a none ```NaN``` entry in the column ```'Response'```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataframe\n",
    "df_ex = df.copy()\n",
    "# map Response to interesting l=classes\n",
    "df_ex['Response'] = df_ex['Response'].map(\n",
    "    {\n",
    "     'C.R.':'negative', \n",
    "     'C.':'negative',\n",
    "#     'Low':'negative',\n",
    "     'Int. I.':'negative',\n",
    "     'Int. II.':'negative',\n",
    "     'Int. II. R.':'negative',\n",
    "     'High':'positive',\n",
    "     'High R.':'negative',\n",
    "    })\n",
    "\n",
    "\n",
    "# drop entries, which do not have a class label (this results in not mapping it to any new target class)\n",
    "# if filter on the column 'target', looking for entries which are None or NaN\n",
    "df_ex = df_ex[df_ex['Response'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the values of all columns\n",
    "\n",
    "Here we use the melt function of pandas. This function allows the values to be plotted in a nice fashion. Just click on Run and see. \n",
    "\n",
    "Are you able to spot an attribute or two, separating positive from negative?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_melt = pd.melt(df_ex,id_vars=\"Response\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.boxplot(x=\"features\", y=\"value\", hue=\"Response\", data=plot_data_melt)\n",
    "ticks_information = plt.xticks(rotation=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Decision Tree Model\n",
    "\n",
    "You might or might not have been able to spot a pattern in the data in order to distinguish positive from negative examples. Here, we build a first decision tree to see what underlying pattern can be found. \n",
    "\n",
    "Before doing this, we split the data into data X and labels y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_ex['Response']\n",
    "X = df_ex.drop(['Response'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "For an initial evaluation of the model, we use a simple train/test split. \n",
    "\n",
    "Please note: **This is only okay for a first  look at the data!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# simple train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the clasifier\n",
    "\n",
    "In sklean, we first have to set up the decision tree model and then train it using our training data. The model expects at least two inputs: the actual data and the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=0)\n",
    "dtree = dt_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the learnt tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is a bit dissapointing. You can use the model to predict, but the printout is not very informative. To overcome this, I have written a plotting function (hidden in the session_helpers import from the beginning).\n",
    "\n",
    "### Plotting the  Tree\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to plot the tree inside the model. This will only work when Graphviz and the pyton module for graphviz are installed. \n",
    "\n",
    "You should see something similar to the following:\n",
    "\n",
    "![2 Class Tree](img/tree_2class.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visulisation:\n",
    "image = session_helpers.plot_tree(dtree,X_test,y_test,rotate=False,max_depth=None)\n",
    "IPython.display.Image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with some of the settings of the decision tree as well as (if you like) with ```rotate``` and ```max_depth``` in the plotting command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more realistic validation scenario: k-fold cross-validation\n",
    "\n",
    "The learning of the tree in the previous sections was only a first glimpse of a validation. Here we use a proper k-fold cross validation to estimate the performance of the learning algorithm. To do this, we need some additional objects (modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "As we do not want to perform the splitting and merging of folds ourselves, we use the prediefined cross validation function in sklearn. \n",
    "\n",
    "Here, we use a simple 5-fold CV. Have a look what other parameters are possible (this might involve you searching the net!)\n",
    "\n",
    "Within each of the folds, we plot the confusion matrix. \n",
    "\n",
    "**Extra  task:**\n",
    "\n",
    "Can you change the code, such that it will calculate the accuracy on each test fold? \n",
    "\n",
    "May be even precision and recall?\n",
    "\n",
    "What is the problem with using ```KFold```? Have a look at this function [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html).\n",
    "\n",
    "**Some information  about  the code:**\n",
    "\n",
    "The python function ```enumerate(...)```, takes an iterable object, such as a list, and combines it with a counter\n",
    "Try out the results  of \n",
    "```python\n",
    "list(enumerate([10,20,30,40]))\n",
    "```\n",
    "PS: The function ```list(...```  coverts an iterable object into a list\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the folds\n",
    "kf = KFold(n_splits=5, random_state=15, shuffle=True)\n",
    "\n",
    "# for loop for each fold\n",
    "for count_k,  (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    # create local datasets\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test  = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test  = y.iloc[test_index]\n",
    "    \n",
    "    # fit the tree\n",
    "    dtree = dt_model.fit(X_train,y_train)\n",
    "    # predict on test fold \n",
    "    y_test_predicted = dtree.predict(X_test)\n",
    "    \n",
    "    # print\n",
    "    print('Confusion Matrix (k={})'.format(count_k))\n",
    "    print(confusion_matrix(y_test,y_test_predicted))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more realistic setting\n",
    "\n",
    "Actually, the data contained more than two classes. In the following, we use a one-to-one mapping. This has the same effect of leaving the 'Response' column unchanged. However, if you would like to delete one group, say 'C.', you can just comment this mapping out (see above with 'Low') \n",
    "\n",
    "Please check the original publication, what the correct mapping would be (look for the section 'Random forest classification')\n",
    "\n",
    "Furthermore, we perform the same kind of analysis as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex = df.copy()\n",
    "df_ex['Response'] = df_ex['Response'].map(\n",
    "    {\n",
    "        'C.':'C.',\n",
    "        'C. R.':'C. R.',\n",
    "        'Low':'Low',\n",
    "        'Int. I.':'Int. I.',\n",
    "        'Int. II.':'Int. II.',\n",
    "        'Int. II. R.':'Int. II. R.',\n",
    "        'High':'High',\n",
    "        'High R.':'High R.',\n",
    "    })\n",
    "\n",
    "df_ex = df_ex[df_ex['Response'].notna()]\n",
    "\n",
    "# For consitency\n",
    "# target column\n",
    "y = df_ex['Response']\n",
    "# this drops the column 'Response' for the dataframe and stores it in X\n",
    "X = df_ex.drop(['Response'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_melt = pd.melt(df_ex,id_vars=\"Response\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(60,10))\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"Response\", data=plot_data_melt)\n",
    "ticks_information = plt.xticks(rotation=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Train/Test - Decision Tree\n",
    "\n",
    "Warning - more than two classes! What does that mean later on?\n",
    "Just in case Graphviz does not work in your setting. Here is the tree I generated:\n",
    "\n",
    "![5 Class Tree](img/tree_5class.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=3)\n",
    "dtree = dt_model.fit(X_train,y_train)\n",
    "# for visulisation:\n",
    "image = session_helpers.plot_tree(dtree,X_test,y_test,rotate=False,max_depth=None)\n",
    "IPython.display.Image(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coss Validation\n",
    "\n",
    "Can you still calculate the accuracy? Again, is KFold the right kind of cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=15, shuffle=True)\n",
    "\n",
    "\n",
    "for count_k,(train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test  = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test  = y.iloc[test_index]\n",
    "    dtree = dt_model.fit(X_train,y_train)\n",
    "    y_test_predicted = dtree.predict(X_test)\n",
    "    print('Confusion Matrix (k={})'.format(count_k))\n",
    "    print(confusion_matrix(y_test,y_test_predicted))\n",
    "    print()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Please scroll down after the last code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, random_state=15, shuffle=True)\n",
    "for count_k,(train_index, test_index) in enumerate(kf.split(X,y)):\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test  = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test  = y.iloc[test_index]\n",
    "    dtree = dt_model.fit(X_train,y_train)\n",
    "    y_test_predicted = dtree.predict(X_test)\n",
    "    print('Confusion Matrix (k={})'.format(count_k))\n",
    "    print(confusion_matrix(y_test,y_test_predicted))\n",
    "    print('Accuracy:     {}'.format(accuracy_score(y_test,y_test_predicted)))\n",
    "    print('F1 Score:     {}'.format(f1_score(y_test,y_test_predicted, average='macro',)))\n",
    "\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Your normal task would be to establish what are the best parameters for each of these folds. Python's sklean offers an easy way to evaluate and test what is the best parameter setting. This way is called grid search. The idea is that you will give a range of hyper-parameters which should be used for testing in the inner loop.  \n",
    "\n",
    "Actually, here we will only do the inner loop on a training and test set setting. Howevewr, you should do this in a real cross validation (outer loop). Furthermore, sklearn cannot easily deal with more than two classes in the grid search and area under curce. Hence, we will be using some form of accuray. Here is a link at possible parameters: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter .\n",
    "\n",
    "To get an idea of what option can be passed as parameter in the grid search, have a look at the decision tree method of sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "In case you get a warning (red messages with DeprecationWarning), please ignore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'criterion':('gini', 'entropy'), \n",
    "    'max_depth':[1,2,3,4],\n",
    "    'min_samples_leaf':[2,5,10]\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=15)\n",
    "\n",
    "dt_grid_search = GridSearchCV(dt_model, parameters, cv=5,scoring='balanced_accuracy') # weighted == F1 Measure for multi-class\n",
    "grid_search = dt_grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of what the grid search returns as information from the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dt_grid_search.cv_results_.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out what the best score was, we can just save the best performace as number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = max(dt_grid_search.cv_results_['mean_test_score'])\n",
    "best_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performing parameter setting\n",
    "\n",
    ".. and now look, which parameter setting performed best with that ```best_result```. \n",
    "\n",
    "Please note that we use here the zip(A,B) method of python to produce a list of tuples from two lists of singletons. I.e. \n",
    "```python \n",
    "zip(['a1','a2','a3'],['b1','b2','b3'])\n",
    "```\n",
    "\n",
    "produces\n",
    "```python \n",
    "[('a1', 'b1'), ('a2', 'b2'), ('a3', 'b3')]\n",
    "```\n",
    "(actually if you want to print this, you will have to put the ```zip()``` into a list : ```list(zip( ... , ... )))```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter_setting, mean_test_score in zip(dt_grid_search.cv_results_['params'],dt_grid_search.cv_results_['mean_test_score']):\n",
    "    if mean_test_score == best_result:\n",
    "        print('-'*80)\n",
    "        print('BEST RESULTS!!')\n",
    "        print(parameter_setting, mean_test_score)\n",
    "        print('-'*80)\n",
    "    else:\n",
    "        print(parameter_setting, mean_test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better way \n",
    "\n",
    "A better way for finding the best performing decision tree, is to directly ask for the best one. Once this is returned, we can use the get_params() method to establish what the set of hyper-parameters were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_model = dt_grid_search.best_estimator_ # best model according to grid search \n",
    "\n",
    "best_tree_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-use the model on another dataset\n",
    "\n",
    "\n",
    "Here we use the 'test set' form the last for loop, just to demonstrate the reuse of the best estimator and predict the dataset of an potentially external test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = best_tree_model.predict(X_test)\n",
    "\n",
    "print('Confusion Matrix of best model on test')\n",
    "print(confusion_matrix(y_test,y_test_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "If you want to find out, what the most influencial attributes (features or biomarker), we can use the the trees built in information about this. \n",
    "\n",
    "Please note that we use the zip(A,B) method of python to produce a list of tuples from two lists of singletons. I.e. \n",
    "```python \n",
    "zip(['a1','a2','a3'],['b1','b2','b3'])\n",
    "```\n",
    "\n",
    "produces\n",
    "```python \n",
    "[('a1', 'b1'), ('a2', 'b2'), ('a3', 'b3')]\n",
    "```\n",
    "(actually if you want to print is, you will have to put the ```zip()``` into a list : ```list(zip( ... , ... )))```\n",
    "\n",
    "Back to feature importance. Have a look at the most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name,feature_importance in zip(X.columns.values,best_tree_model.feature_importances_):\n",
    "    if feature_importance > 0.0:\n",
    "        print('{:20s}:{:3.4f}'.format(feature_name,feature_importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Let us repeat this exercise with Random Forrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [2,3,5], \n",
    "    'max_depth':[1,2,3,4],\n",
    "    'min_samples_leaf':[2,5,10]\n",
    "}\n",
    "\n",
    "random_f_model = RandomForestClassifier() \n",
    "rf_grid_search = GridSearchCV(random_f_model, parameters, cv=5,scoring='balanced_accuracy') # weighted == F1 Measure for multi-class\n",
    "grid_search = rf_grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_f_model = rf_grid_search.best_estimator_ # best model according to grid search \n",
    "\n",
    "best_random_f_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-use the model on another dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = best_random_f_model.predict(X_test)\n",
    "\n",
    "print('Confusion Matrix of best model on test')\n",
    "print(confusion_matrix(y_test,y_test_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important biomarkers\n",
    "\n",
    "The first version uses a for loop and if statement. You can also save the results of this in a pandas dataframe (see below), which furthermore allows to simply sort the reults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name,feature_importance in zip(X.columns.values,best_random_f_model.feature_importances_):\n",
    "    if feature_importance > 0.0:\n",
    "        print('{:20s}:{:3.4f}'.format(feature_name,feature_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dataframes\n",
    "df_importance = pd.DataFrame(list(zip(X_test.columns.values,best_random_f_model.feature_importances_)),columns=['column_name','feature_importance'])\n",
    "df_importance = df_importance.set_index(['column_name'])\n",
    "df_importance.sort_values(['feature_importance'],ascending=False,inplace=True)\n",
    "\n",
    "#df_importance[df_importance['feature_importance']>0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "sns.barplot(x='column_name',y='feature_importance',data=df_importance.reset_index(),palette='muted')\n",
    "ticks_information = plt.xticks(rotation=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27504229a786ecc4bac7f6801848e44cb1a63648e4d666cbb9fdea5e9f215e02"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
